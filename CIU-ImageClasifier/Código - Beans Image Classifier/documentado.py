# -*- coding: utf-8 -*-
"""Documentado

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18dMoCqe0xShdzouuySBrwKw7wrg2QaRX
"""

#Importamos librerias necesarias para manejo de datos
import matplotlib.pylab as plt
import tensorflow as tf
import tensorflow_hub as hub

import os
import numpy as np

import tensorflow_datasets as tfds

import warnings
warnings.filterwarnings('ignore')

#Descargamos el dataset desde TensoFlow separando los datos en train, test y validation
datasets , info = tfds.load(name = 'beans', with_info = True, as_supervised = True, split = ['train', 'test', 'validation'])

#Informacion del dataset
info

#Mostramos los datasets divididos
datasets

#Asignamos a Train la division de los datos test del dataset
#Asignamos a info_train la division de la label de test del dataset
train, info_train = tfds.load(name = 'beans', with_info = True, split = 'test')

#Mostramos los datos de Train = split de test del dataset
tfds.show_examples(train, info_train)

#CREAMOS LA FUNCION SCALE PARA USAR MAS ABAJO RECIBE UNA IMAGEN Y UNA ETIQUETA

def scale(image, label):
    image = tf.cast(image, tf.float32) #Cambiamos el tipo al float32
    image /= 255.0 #Normalizamos los pixeles /255 para tener valores de 0 - 1
    
    return tf.image.resize(image, [224, 224]), tf.one_hot(label, 3) 
    #Escalamos las imagenes a 224x224 como lo pide el modelo mobilnet2
    # el 3 es por los 3 canales si usaramos en blanco y negro cambiar a 1

#CREAMOS LA FUNCION Get_Dataset que recibe datos de 32 en 32

def get_dataset(batch_size = 32):
    train_dataset_sclaed = datasets[0].map(scale).shuffle(1000).batch(batch_size) # datasets[0] = Train dataset
    test_dataset_sclaed = datasets[1].map(scale).batch(batch_size) # datasets[1] = Test dataset
    validation_dataset_sclaed = datasets[2].map(scale).batch(batch_size) # datasets[2] = Validation dataset
    
    return train_dataset_sclaed, test_dataset_sclaed, validation_dataset_sclaed # Regresamos los dataset escalados

train_dataset, test_dataset, val_dataset = get_dataset() #Creamos nuestros datasets usando las dos funciones previas

train_dataset.cache() #Visualizamos que todo este correcto en dimensiones y tipos
val_dataset.cache()

#Visualizamos que el numero de datos este correcto para el train 
len(list(datasets[0]))

#Cargamos el modelo mobilnet
feature_extractor = 'https://tfhub.dev/google/tf2-preview/mobilenet_v2/feature_vector/4'

#Damos la forma de entrada
feature_extractor_layer = hub.KerasLayer(feature_extractor, input_shape = (224, 224, 3))

feature_extractor_layer.trainable = False

#Creacion del Modelo usando el feature extractor del mobilnet
model = tf.keras.Sequential(
    [
        feature_extractor_layer,
        tf.keras.layers.Dropout(0.3),
        tf.keras.layers.Dense(3, activation = 'softmax') # el 3 es de 3 clases de salida
    ]
)

#informacion del modelo
model.summary()

#complimos el modelo
model.compile(
    optimizer = tf.keras.optimizers.Adam(),
    loss = tf.keras.losses.CategoricalCrossentropy(from_logits = True),
    metrics = ['acc']
)

# entrenamos el modelo 
hist = model.fit(train_dataset, epochs = 6, validation_data = val_dataset)

#Evaluamos el modelo
result = model.evaluate(test_dataset)

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

test_sample[0]

# datasets[1] = Test dataset

for test_sample in datasets[1].take(10):#Creamos una variable test_sample = a el test dataset con 10 imagenes 500x500 (Originales)
    image, label = test_sample[0], test_sample[1] #separamos imagen y eqtiqueta #datasets[1] de arriba en la vaiable test_sample = datasets[1] = Train dataset original
    image_scaled, label_arr = scale(test_sample[0], test_sample[1]) #usamos nuevamente la funcion escala de arriba
    image_scaled = np.expand_dims(image_scaled, axis = 0) #a√±adimos una dimension al array
    
    img = tf.keras.preprocessing.image.img_to_array(image) #img = imagenes del test_sample[0] = imagebes del Test dataset originales 500x500
    
    pred = model.predict(image_scaled) #Con este obtenemos los porcentajes
    print(pred) #Imprimimos los porcentajes
    
    plt.figure()
    plt.imshow(image)
    plt.show()
    
    print("Actual Label : %s" %info.features['label'].names[label.numpy()]) #label contiene etiqueta del test dataset original
    print("Predicted Label : %s" %info.features['label'].names[np.argmax(pred)])#argmax da el mas alto del label de acuerdo a la prediccion

#creamos la matriz de confusion
for f0, f1 in datasets[1].map(scale).batch(200):
    y = np.argmax(f1, axis = 1)
    y_pred = np.argmax(model.predict(f0), axis = 1)
    
    print(tf.math.confusion_matrix(labels = y, predictions = y_pred, num_classes = 3))

from tensorflow.keras.callbacks import TensorBoard

tensorboardDenso = TensorBoard(log_dir='logs/denso')
hist = model.fit(train_dataset, epochs = 6, validation_data = val_dataset,callbacks=[tensorboardDenso])

# Commented out IPython magic to ensure Python compatibility.
#Cargar la extension de tensorboard de colab
# %load_ext tensorboard

# Commented out IPython magic to ensure Python compatibility.
#Ejecutar tensorboard e indicarle que lea la carpeta "logs"
# %tensorboard --logdir logs